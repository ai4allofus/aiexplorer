services:
  llms:
    image: aicrazyguy/ai4allofus:86cuda124
    container_name: llmserver
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - "./cache:/root/cache"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
  promptedit:
    image: aicrazyguy/promptedit:v0.1
    container_name: golem
    restart: unless-stopped
    ports:
      - "3000:3000"

